Копия этого файла лежит в SentimentAnalysis вместе со всеми данными

Naive Bayesian классификатор
Мы пытаемся понимать что-то про Почту России. Уже сразу понятно, что сильно хороших результатов добиться не удастся — слишком много всякого сарказма с ней связано.
Учимся — набор твитов на русском языке, собранных через streaming api по санкт-петербургу за примерно 1.5 месяца. Из них ищем твиты, содяржащие смайлы и делаем датасет для обучения
В итоге имеем такое кол-во «хороших» и «плохих» твитов — перекос в сторону хороших, а для почты россии будет наоборот.
wc -l happy_tweets
   12065 happy_tweets
wc -l sad_tweets
   2122 sad_tweets

Обучаемся. Провяряем на размеченных твитах с почтой россии:

     precision: 0.23369565217391305
     accuracy: 0.3393665158371041
     recall: 0.8958333333333334
     F: 0.4922536823628177

Выводы:

Как и предполагалось — все достаточно печально. Причины — на самом деле у нас мало данных, твиттер очень специфичен, а в комбинации с русским языком вообще получаем, что один и тот же твит
можно трактовать несколькими различными способами. Вдобавок использовали только слова +  naive bayes  cам по себе достаточно плохой классификатор (особенно если данных мало)

Часть 2:

Словарь оценочных слов — составляем по PMI = #{word & label in corpus} / #{word in corpus}
Ну данных у нас мало, еще и твиттер…

Выписать топ-200 наиболее «негативных» и топ-200 «позитивных» слов по одной из формул для выбора атрибутов в соответствующих твитах — имеем кучу слов, которые встречаются только в каком-то из классов, но
которые никак на самом деле не связаны с классом. Вдобавок позитивные и негативные смайлы — плохой индикатор содержимого твита… Но раз есть в задание — сделали. Вместо 200 — отранжировали все
слова, которые встречались и в положительных твитат, и в отрицательных, по PMI и среди ≈500 первых — выбрали те, которые являются «негативными» или «позитивными».
 Осталось примерно по 70 штук в каждой категории

Уже сразу при  ручной обработке было понятно, что ничего не заработает — совпадения по словам между твитами про почту России и теми словами, которые получились как негативные и позитивные
было практически нулевое.

Запустили и увидели результаты, которые и ожидалось увидеть:

    precision: 0.0
    accuracy: 0.7420814479638009
    recall: 0.0
    F: 0.0

Выводы:
 1) Сарказм с помощью словаря не уловишь, а большинство твитов про «Почта России» имено такие
 2) Данных мало
 3) В очередной раз — в русском языке слишком много словоформ
 4) У нас плохой словарь с малым кол-вом слов


 Files description:
     happy_posts, sad_posts — хорошие и плохие мнения о почте россии
     model — файл с вероятностями нашего bayesian classifier
     sad_tweets, happy_tweets — хорошие и плохие твиты (полученные на основе смайлов)
     bad_classifier, good_classifier — слова и токены, размеченные вручную
     badPMI, goodPMI — рейтинг токенов (слов), полученный по данным из твиттера

 Файла, из которого брались твиты нету, т.к. он занимает 1Gb
 Сортировка по PMI и некоторые другие действия проводились в интерактивном режиме в ipython (TweetsExtractor.ipynb)
